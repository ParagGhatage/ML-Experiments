{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/paragghatage/understanding-llms?scriptVersionId=260336700\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7f88e98d","metadata":{"papermill":{"duration":0.004078,"end_time":"2025-09-06T14:22:20.44968","exception":false,"start_time":"2025-09-06T14:22:20.445602","status":"completed"},"tags":[]},"source":["## What is AI ?\n","the science and engineering of making\n","intelligent machines, especially intelligent computer programs. It is\n","related to the similar task of using computers to understand human\n","intelligence, but AI does not have to confine itself to methods that are\n","biologically observable.\n","                        - John McCarthy, 2007"]},{"cell_type":"markdown","id":"dd203fa5","metadata":{"papermill":{"duration":0.003076,"end_time":"2025-09-06T14:22:20.456387","exception":false,"start_time":"2025-09-06T14:22:20.453311","status":"completed"},"tags":[]},"source":["## Bag of Words-\n","it was the intial technique of how models used to understand language."]},{"cell_type":"markdown","id":"bb7a67f7","metadata":{"papermill":{"duration":0.002867,"end_time":"2025-09-06T14:22:20.462639","exception":false,"start_time":"2025-09-06T14:22:20.459772","status":"completed"},"tags":[]},"source":["## 1. Tokenization- splitting sentences into words seperated by spaces."]},{"cell_type":"markdown","id":"d6b73ebe","metadata":{"papermill":{"duration":0.002913,"end_time":"2025-09-06T14:22:20.468745","exception":false,"start_time":"2025-09-06T14:22:20.465832","status":"completed"},"tags":[]},"source":["eg. \n","\n","Bear is huge   -->   [\"Bear\", \"is\", \"huge\"]"]},{"cell_type":"markdown","id":"9266356c","metadata":{"papermill":{"duration":0.002909,"end_time":"2025-09-06T14:22:20.474855","exception":false,"start_time":"2025-09-06T14:22:20.471946","status":"completed"},"tags":[]},"source":["Then we create a vocabulary the combines all unique words from all the sentences.\n","\n","eg.\n","\n","I have a cat.My cat is really cute. --> vocabulary = [I, have,a ,cat,My, is, really, cute]  (No duplicates)"]},{"cell_type":"markdown","id":"516b8dd1","metadata":{"papermill":{"duration":0.002888,"end_time":"2025-09-06T14:22:20.480835","exception":false,"start_time":"2025-09-06T14:22:20.477947","status":"completed"},"tags":[]},"source":["## Vector Representation-\n","it counts how many times word from vocabulary is repeated in input snetence \n","eg. I love cycling and programming\n","\n","vector = [1,0,0,0,0,0,0,0]\n","\n","because only I is present in input sentence which has frequency of 1"]},{"cell_type":"markdown","id":"e47b8812","metadata":{"papermill":{"duration":0.002774,"end_time":"2025-09-06T14:22:20.48679","exception":false,"start_time":"2025-09-06T14:22:20.484016","status":"completed"},"tags":[]},"source":["But , It never understands semantic meaning of sentences and words."]},{"cell_type":"markdown","id":"8b6b35c5","metadata":{"papermill":{"duration":0.00291,"end_time":"2025-09-06T14:22:20.49281","exception":false,"start_time":"2025-09-06T14:22:20.4899","status":"completed"},"tags":[]},"source":["## Word2Vec- One of the very first model to understand semantic meaning of words.\n","\n","it is neural network trained on huge language data and understand their sematic meaning between words.\n","\n","### Embedding - Embedding are vector reresentation of data that captures their meaning.\n","Word2Vec initializes random weights, then based on training data, it compares 2 words at a time based on similarity, like, how much similar these two words are?\n","\n","so, for input cat and cute output will be similarity between two words-->0.79"]},{"cell_type":"markdown","id":"9d208c9c","metadata":{"papermill":{"duration":0.002808,"end_time":"2025-09-06T14:22:20.498816","exception":false,"start_time":"2025-09-06T14:22:20.496008","status":"completed"},"tags":[]},"source":["## Embedding types- Sentence embedding, word embedding.\n","Bag-of-words creates document-level embedding"]},{"cell_type":"markdown","id":"0047d09a","metadata":{"papermill":{"duration":0.00287,"end_time":"2025-09-06T14:22:20.505136","exception":false,"start_time":"2025-09-06T14:22:20.502266","status":"completed"},"tags":[]},"source":["## Encoding- Converting text to embedding\n","## Decoding- converting embedding to text\n","## Attention mechanism- rnn pays attention to specific part of the input at right time."]},{"cell_type":"markdown","id":"f4f2e5ce","metadata":{"papermill":{"duration":0.003539,"end_time":"2025-09-06T14:22:20.512647","exception":false,"start_time":"2025-09-06T14:22:20.509108","status":"completed"},"tags":[]},"source":["# Attention is all you need-\n","\n","The authors proposed a network architecture\n","called the Transformer, which was solely based on the attention mechanism\n","and removed the recurrence network that we saw previously. Compared to\n","the recurrence network, the Transformer could be trained in parallel, which\n","tremendously sped up training."]},{"cell_type":"markdown","id":"688144a5","metadata":{"papermill":{"duration":0.003154,"end_time":"2025-09-06T14:22:20.520206","exception":false,"start_time":"2025-09-06T14:22:20.517052","status":"completed"},"tags":[]},"source":["# Transformers- \n","In the transformer, there are 2 main layers- encoder and decoder.\n","each encoder has multiple encoders with self-attention to get better idea for generattion, same for decoder- each decoder has multiple decoders stacked on each other."]},{"cell_type":"markdown","id":"99e3cd81","metadata":{"papermill":{"duration":0.002842,"end_time":"2025-09-06T14:22:20.526289","exception":false,"start_time":"2025-09-06T14:22:20.523447","status":"completed"},"tags":[]},"source":["each small Encoder has 2 layers- Self attention layer and feed forward neural network.\n","so, for each word,embedding is calculated and self attention goes over emtire sentence and improves the mebedding based on sentence contex.\n","feed forward neural network just optimizes embedding of each encoder."]},{"cell_type":"markdown","id":"4d5de044","metadata":{"papermill":{"duration":0.002869,"end_time":"2025-09-06T14:22:20.532254","exception":false,"start_time":"2025-09-06T14:22:20.529385","status":"completed"},"tags":[]},"source":["Also for decoder, it has 3 layers- Masked self attenntion, encoder attention,feed forward neural network.\n"]},{"cell_type":"markdown","id":"3d099e89","metadata":{"papermill":{"duration":0.002834,"end_time":"2025-09-06T14:22:20.538124","exception":false,"start_time":"2025-09-06T14:22:20.53529","status":"completed"},"tags":[]},"source":["## Representation Models-(Encoder only)"]},{"cell_type":"markdown","id":"fc3a977e","metadata":{"papermill":{"duration":0.00283,"end_time":"2025-09-06T14:22:20.544051","exception":false,"start_time":"2025-09-06T14:22:20.541221","status":"completed"},"tags":[]},"source":["## BERT- encoder only model that consists of 12 encoder layers.\n","\n"]},{"cell_type":"markdown","id":"9b380c48","metadata":{"papermill":{"duration":0.002827,"end_time":"2025-09-06T14:22:20.549916","exception":false,"start_time":"2025-09-06T14:22:20.547089","status":"completed"},"tags":[]},"source":["Each transformer encoder has self attention layer and feed forward neural network."]},{"cell_type":"markdown","id":"5ae8ea0a","metadata":{"papermill":{"duration":0.002919,"end_time":"2025-09-06T14:22:20.555892","exception":false,"start_time":"2025-09-06T14:22:20.552973","status":"completed"},"tags":[]},"source":["BERT uses masked language modelling to understand bidirectional context"]},{"cell_type":"markdown","id":"15f2418c","metadata":{"papermill":{"duration":0.002841,"end_time":"2025-09-06T14:22:20.561851","exception":false,"start_time":"2025-09-06T14:22:20.55901","status":"completed"},"tags":[]},"source":["# Generativ models- Decoder only models\n","it has 12 decoders stacked, each has masked self attention and FFNN layers.\n"]},{"cell_type":"markdown","id":"497082b2","metadata":{"papermill":{"duration":0.002847,"end_time":"2025-09-06T14:22:20.568027","exception":false,"start_time":"2025-09-06T14:22:20.56518","status":"completed"},"tags":[]},"source":["## GPT-1: Generative Pre-trained Transformer.\n"]},{"cell_type":"markdown","id":"8291cc24","metadata":{"papermill":{"duration":0.002918,"end_time":"2025-09-06T14:22:20.573994","exception":false,"start_time":"2025-09-06T14:22:20.571076","status":"completed"},"tags":[]},"source":["It had 117 M params.\n","GPT-2 had 1.5 B params and GPT3 had 175 Billion params."]},{"cell_type":"markdown","id":"91967bec","metadata":{"papermill":{"duration":0.002994,"end_time":"2025-09-06T14:22:20.580278","exception":false,"start_time":"2025-09-06T14:22:20.577284","status":"completed"},"tags":[]},"source":["## LLM training steps-\n","\n","1. ### Language modeling or pretraining  - in this step, we basically train llm from scratch on vast data, not specififc at all, LLM just learns topredict next word, Resulting model is called Base Model, ususally base models dont follow instructions.\n","\n","2. ### Fine Tuning- We train base mnodel on our specific narrow data, so, model adapts and becomes better in specific task like classification, generating code, or follow instructions."]},{"cell_type":"code","execution_count":null,"id":"79c761d4","metadata":{"papermill":{"duration":0.002877,"end_time":"2025-09-06T14:22:20.586259","exception":false,"start_time":"2025-09-06T14:22:20.583382","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":6.235976,"end_time":"2025-09-06T14:22:21.009035","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-06T14:22:14.773059","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}